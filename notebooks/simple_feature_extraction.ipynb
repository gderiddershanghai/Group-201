{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4886d71a-b613-434b-a1e2-e0ab12c09ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Folder Structure:\n",
    "# simple_feature_extraction.ipynb (this script)\n",
    "\n",
    "# asap-aes (folder)\n",
    "## training_set_rel3.tsv\n",
    "\n",
    "# supplementary_data\n",
    "## Kuperman-BRM-data-2012.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f3c7dd-8201-446c-9679-0ec359acd4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/yli/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/yli/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "/Users/yli/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import scipy\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import warnings\n",
    "import textstat\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "import pyphen\n",
    "import syllapy\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import spacy\n",
    "#python3 -m spacy download en_core_web_md\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "# import gingerit\n",
    "# import languagetool_python\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f97390-d8a6-42cb-9cb0-22d5806d6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_set = pd.read_csv('asap-aes/training_set_rel3.tsv',sep='\\t', encoding='latin1')\n",
    "aoa_df = pd.read_csv('supplementary_data/Kuperman-BRM-data-2012.csv') # age of acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d759849e-0927-415d-9d23-b0bdf817b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12976, 28)\n",
      "Index(['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1',\n",
      "       'rater3_domain1', 'domain1_score', 'rater1_domain2', 'rater2_domain2',\n",
      "       'domain2_score', 'rater1_trait1', 'rater1_trait2', 'rater1_trait3',\n",
      "       'rater1_trait4', 'rater1_trait5', 'rater1_trait6', 'rater2_trait1',\n",
      "       'rater2_trait2', 'rater2_trait3', 'rater2_trait4', 'rater2_trait5',\n",
      "       'rater2_trait6', 'rater3_trait1', 'rater3_trait2', 'rater3_trait3',\n",
      "       'rater3_trait4', 'rater3_trait5', 'rater3_trait6'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(raw_training_set.shape)\n",
    "print(raw_training_set.columns)\n",
    "#raw_training_set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a849832b-90be-4162-bc9f-e6ae958ddff1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "essay_id          False\n",
       "essay_set         False\n",
       "essay             False\n",
       "rater1_domain1    False\n",
       "rater2_domain1    False\n",
       "rater3_domain1     True\n",
       "domain1_score     False\n",
       "rater1_domain2     True\n",
       "rater2_domain2     True\n",
       "domain2_score      True\n",
       "rater1_trait1      True\n",
       "rater1_trait2      True\n",
       "rater1_trait3      True\n",
       "rater1_trait4      True\n",
       "rater1_trait5      True\n",
       "rater1_trait6      True\n",
       "rater2_trait1      True\n",
       "rater2_trait2      True\n",
       "rater2_trait3      True\n",
       "rater2_trait4      True\n",
       "rater2_trait5      True\n",
       "rater2_trait6      True\n",
       "rater3_trait1      True\n",
       "rater3_trait2      True\n",
       "rater3_trait3      True\n",
       "rater3_trait4      True\n",
       "rater3_trait5      True\n",
       "rater3_trait6      True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_set.isna().any(axis = 0)\n",
    "#raw_training_set.isna().all(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b70870c-5fa2-4147-9663-ac5cf9136472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dear @CAPS1 @CAPS2, I believe that using computers will benefit us in many ways like talking and becoming friends will others through websites like facebook and mysace. Using computers can help us find coordibates, locations, and able ourselfs to millions of information. Also computers will benefit us by helping with jobs as in planning a house plan and typing a @NUM1 page report for one of our jobs in less than writing it. Now lets go into the wonder world of technology. Using a computer will help us in life by talking or making friends on line. Many people have myspace, facebooks, aim, these all benefit us by having conversations with one another. Many people believe computers are bad but how can you make friends if you can never talk to them? I am very fortunate for having a computer that can help with not only school work but my social life and how I make friends. Computers help us with finding our locations, coordibates and millions of information online. If we didn't go on the internet a lot we wouldn't know how to go onto websites that @MONTH1 help us with locations and coordinates like @LOCATION1. Would you rather use a computer or be in @LOCATION3. When your supposed to be vacationing in @LOCATION2. Million of information is found on the internet. You can as almost every question and a computer will have it. Would you rather easily draw up a house plan on the computers or take @NUM1 hours doing one by hand with ugly erazer marks all over it, you are garrenteed that to find a job with a drawing like that. Also when appling for a job many workers must write very long papers like a @NUM3 word essay on why this job fits you the most, and many people I know don't like writing @NUM3 words non-stopp for hours when it could take them I hav an a computer. That is why computers we needed a lot now adays. I hope this essay has impacted your descion on computers because they are great machines to work with. The other day I showed my mom how to use a computer and she said it was the greatest invention sense sliced bread! Now go out and buy a computer to help you chat online with friends, find locations and millions of information on one click of the button and help your self with getting a job with neat, prepared, printed work that your boss will love.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_str = raw_training_set['essay'][1]\n",
    "\n",
    "#essay_str.split(' ')\n",
    "essay_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9be3ee55-5753-40c5-ae9b-14eec1b37e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sets(essay_str):\n",
    "\n",
    "    words = essay_str.split(' ')\n",
    "    words = [w for w in words if len(w) > 0]\n",
    "    words = [w for w in words if w[0] != '@']\n",
    "    words = [w.lower() for w in words]\n",
    "    #tokens = word_tokenize(essay_str)\n",
    "    \n",
    "    idx_set1 = math.floor(len(words) / 3)\n",
    "    idx_set2 = idx_set1 + idx_set1\n",
    "    \n",
    "    text_set1, text_set2, text_set3 = words[:idx_set1], words[idx_set1:idx_set2], words[idx_set2:]\n",
    "    text_set1, text_set2, text_set3 = ' '.join(text_set1), ' '.join(text_set2), ' '.join(text_set3)\n",
    "    #print(len(text_set1), len(text_set2), len(text_set3))\n",
    "\n",
    "    return text_set1, text_set2, text_set3\n",
    "\n",
    "\n",
    "def remove_punctuations(x):\n",
    "    return [re.sub(r'[^\\w\\s]', '', token) for token in x if re.sub(r'[^\\w\\s]', '', token)]\n",
    "\n",
    "def get_avg_AoA(tokens):\n",
    "    tokens_df = pd.DataFrame({'token': tokens})\n",
    "    tokens_df = pd.merge(tokens_df, aoa_df[['Word', 'Rating.Mean']], left_on = 'token', right_on = 'Word', how = 'left')\n",
    "    \n",
    "    if tokens_df['Rating.Mean'].isna().all():\n",
    "        avg_aoa = 0\n",
    "    else:\n",
    "        avg_aoa = tokens_df['Rating.Mean'].mean()\n",
    "\n",
    "    return avg_aoa\n",
    "\n",
    "def get_cohesion_score(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # obtain vectors of size (300, ) for each word in the text\n",
    "    vec_tokens = [token.vector for token in doc if token.has_vector and not token.is_stop and not token.is_punct]\n",
    "\n",
    "    # calculate cosine similarity score for combinations of two vectors\n",
    "    similarities = [np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "                    for v1, v2 in combinations(vec_tokens, 2)]\n",
    "    \n",
    "    if (len(vec_tokens) < 2) or (len(similarities) > 0):\n",
    "        return np.mean(similarities)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f28432ef-2c47-4843-ae46-06c9d9b9370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_training_df = raw_training_set[['essay_id', 'essay_set', 'essay']].copy()\n",
    "text_training_df = raw_training_set[['essay_id', 'essay_set', 'essay']].head(200).copy() # take the first 200 rows to prototype faster\n",
    "\n",
    "# split each essay into 3 sets, and keep the original essay labeled as \"text_original\"\n",
    "text_training_df[['text_set1', 'text_set2', 'text_set3']] = text_training_df['essay'].apply(lambda x: pd.Series(get_sets(x)))\n",
    "text_training_df = text_training_df.rename(columns = {'essay': 'text_original'})\n",
    "text_training_df = text_training_df.melt(id_vars=['essay_id', 'essay_set'], value_vars=['text_set1', 'text_set2', 'text_set3', 'text_original'],\n",
    "                                        var_name='text_set', value_name='text')\n",
    "\n",
    "# tokenize into words and sentences\n",
    "text_training_df['word_tokens'] = text_training_df['text'].apply(lambda x: word_tokenize(x))\n",
    "text_training_df['sent_tokens'] = text_training_df['text'].apply(lambda x: sent_tokenize(x))\n",
    "text_training_df['word_tokens_clean'] = text_training_df['word_tokens'].apply(lambda x: remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0dda90f9-cf77-4a42-b39a-76f44f0ecfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.53421 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>text_set</th>\n",
       "      <th>text</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>word_tokens_clean</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>spell_err_count</th>\n",
       "      <th>syllabus_count</th>\n",
       "      <th>FleKin_score</th>\n",
       "      <th>DalCha_score</th>\n",
       "      <th>unique_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>text_set1</td>\n",
       "      <td>dear local newspaper, i think effects computer...</td>\n",
       "      <td>[dear, local, newspaper, ,, i, think, effects,...</td>\n",
       "      <td>[dear local newspaper, i think effects compute...</td>\n",
       "      <td>[dear, local, newspaper, i, think, effects, co...</td>\n",
       "      <td>112</td>\n",
       "      <td>7</td>\n",
       "      <td>624</td>\n",
       "      <td>16.000</td>\n",
       "      <td>8</td>\n",
       "      <td>152</td>\n",
       "      <td>78.28</td>\n",
       "      <td>8.13</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>text_set1</td>\n",
       "      <td>dear i believe that using computers will benef...</td>\n",
       "      <td>[dear, i, believe, that, using, computers, wil...</td>\n",
       "      <td>[dear i believe that using computers will bene...</td>\n",
       "      <td>[dear, i, believe, that, using, computers, wil...</td>\n",
       "      <td>136</td>\n",
       "      <td>8</td>\n",
       "      <td>767</td>\n",
       "      <td>17.000</td>\n",
       "      <td>6</td>\n",
       "      <td>204</td>\n",
       "      <td>62.68</td>\n",
       "      <td>7.50</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>text_set1</td>\n",
       "      <td>dear, more and more people use computers, but ...</td>\n",
       "      <td>[dear, ,, more, and, more, people, use, comput...</td>\n",
       "      <td>[dear, more and more people use computers, but...</td>\n",
       "      <td>[dear, more, and, more, people, use, computers...</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "      <td>519</td>\n",
       "      <td>13.000</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>75.30</td>\n",
       "      <td>7.61</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>text_set1</td>\n",
       "      <td>dear local newspaper, i have found that many e...</td>\n",
       "      <td>[dear, local, newspaper, ,, i, have, found, th...</td>\n",
       "      <td>[dear local newspaper, i have found that many ...</td>\n",
       "      <td>[dear, local, newspaper, i, have, found, that,...</td>\n",
       "      <td>161</td>\n",
       "      <td>8</td>\n",
       "      <td>922</td>\n",
       "      <td>20.125</td>\n",
       "      <td>7</td>\n",
       "      <td>246</td>\n",
       "      <td>59.53</td>\n",
       "      <td>8.65</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>text_set1</td>\n",
       "      <td>dear i know having computers has a positive ef...</td>\n",
       "      <td>[dear, i, know, having, computers, has, a, pos...</td>\n",
       "      <td>[dear i know having computers has a positive e...</td>\n",
       "      <td>[dear, i, know, having, computers, has, a, pos...</td>\n",
       "      <td>153</td>\n",
       "      <td>9</td>\n",
       "      <td>823</td>\n",
       "      <td>17.000</td>\n",
       "      <td>2</td>\n",
       "      <td>225</td>\n",
       "      <td>71.14</td>\n",
       "      <td>6.85</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set   text_set  \\\n",
       "0         1          1  text_set1   \n",
       "1         2          1  text_set1   \n",
       "2         3          1  text_set1   \n",
       "3         4          1  text_set1   \n",
       "4         5          1  text_set1   \n",
       "\n",
       "                                                text  \\\n",
       "0  dear local newspaper, i think effects computer...   \n",
       "1  dear i believe that using computers will benef...   \n",
       "2  dear, more and more people use computers, but ...   \n",
       "3  dear local newspaper, i have found that many e...   \n",
       "4  dear i know having computers has a positive ef...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [dear, local, newspaper, ,, i, think, effects,...   \n",
       "1  [dear, i, believe, that, using, computers, wil...   \n",
       "2  [dear, ,, more, and, more, people, use, comput...   \n",
       "3  [dear, local, newspaper, ,, i, have, found, th...   \n",
       "4  [dear, i, know, having, computers, has, a, pos...   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [dear local newspaper, i think effects compute...   \n",
       "1  [dear i believe that using computers will bene...   \n",
       "2  [dear, more and more people use computers, but...   \n",
       "3  [dear local newspaper, i have found that many ...   \n",
       "4  [dear i know having computers has a positive e...   \n",
       "\n",
       "                                   word_tokens_clean  word_count  sent_count  \\\n",
       "0  [dear, local, newspaper, i, think, effects, co...         112           7   \n",
       "1  [dear, i, believe, that, using, computers, wil...         136           8   \n",
       "2  [dear, more, and, more, people, use, computers...          91           7   \n",
       "3  [dear, local, newspaper, i, have, found, that,...         161           8   \n",
       "4  [dear, i, know, having, computers, has, a, pos...         153           9   \n",
       "\n",
       "   char_count  sent_length  spell_err_count  syllabus_count  FleKin_score  \\\n",
       "0         624       16.000                8             152         78.28   \n",
       "1         767       17.000                6             204         62.68   \n",
       "2         519       13.000                2             136         75.30   \n",
       "3         922       20.125                7             246         59.53   \n",
       "4         823       17.000                2             225         71.14   \n",
       "\n",
       "   DalCha_score  unique_word_count  \n",
       "0          8.13                 75  \n",
       "1          7.50                 90  \n",
       "2          7.61                 70  \n",
       "3          8.65                107  \n",
       "4          6.85                 90  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract basic statistical features\n",
    "# (1) total number of words \n",
    "# (2) total number of characters\n",
    "# (3) average number of words per sentence\n",
    "# (4) total number of sentences\n",
    "# (5) total number of paragraphs, ---> I don't think we can do this, doesn't seem to be in the raw data\n",
    "# (6) total number of spelling mistakes\n",
    "# (7) total number of grammar mistakes ---> I tried multiple packages but no luck. Textblob and transformer worked, but test cases are wrong. Skipped for now. \n",
    "# Flesch-Kincaid Score\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "text_training_df['word_count'] = text_training_df['word_tokens_clean'].apply(lambda x: len(x))\n",
    "text_training_df['sent_count'] = text_training_df['sent_tokens'].apply(lambda x: len(x))\n",
    "text_training_df['char_count'] = text_training_df['text'].apply(lambda x: len(x))\n",
    "text_training_df['sent_length'] = text_training_df['word_count'] / text_training_df['sent_count']\n",
    "\n",
    "spellcheck = SpellChecker()\n",
    "text_training_df['spell_err_count'] = text_training_df['word_tokens_clean'].apply(lambda x: len(spellcheck.unknown(x)))\n",
    "text_training_df['syllabus_count'] = text_training_df['word_tokens_clean'].apply(lambda x: sum(syllapy.count(word) for word in x))\n",
    "#text_training_df['FleKin_score'] = (0.39 * (text_training_df['word_count'] / text_training_df['sent_count'])) + (11.8 * (text_training_df['syllabus_count'] / text_training_df['word_count'])) - 15.59\n",
    "# the scores generated from this formula is very different from using flesch_reading_ease\n",
    "# the formula is also different from wikipedia. Where did you get this?\n",
    "\n",
    "# text_training_df['FleKin_score'] = 206.835 - (1.015 * (text_training_df['word_count'] / text_training_df['sent_count'])) - (84.6 * (text_training_df['syllabus_count'] / text_training_df['word_count']))\n",
    "# this formula is from Wikipedia and close to the result of using flesch_reading_ease\n",
    "\n",
    "text_training_df['FleKin_score'] = text_training_df['text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "\n",
    "# extras, not in the proposal\n",
    "text_training_df['DalCha_score'] = text_training_df['text'].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "text_training_df['unique_word_count'] = text_training_df['word_tokens_clean'].apply(lambda x: len(set(w.lower() for w in x)))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Time taken for basic statistical feature extraction: {execution_time:.5f} seconds\")\n",
    "\n",
    "text_training_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae25a0c9-4769-4fb4-8b11-86f8292ea1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mauryquijada/word-complexity-predictor/tree/master\n",
    "# Supposed to be able to extract these features using machine learning techniques:\n",
    "    # Lemma length\n",
    "    # Average age-of-acquisition (at what age a word is most likely to enter someone's vocabulary)\n",
    "    # Average concreteness (a score of 1 to 5, with 5 being very concrete)\n",
    "    # Frequency in a certain corpus\n",
    "    # Lemma frequency in a certain corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b6689db-1c43-4852-9b6d-59f4b4acec2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for basic content feature extraction: 34.51738 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>text_set</th>\n",
       "      <th>text</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>word_tokens_clean</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>...</th>\n",
       "      <th>syllabus_count</th>\n",
       "      <th>FleKin_score</th>\n",
       "      <th>DalCha_score</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>AoA_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>text_set1</td>\n",
       "      <td>dear local newspaper, i think effects computer...</td>\n",
       "      <td>[dear, local, newspaper, ,, i, think, effects,...</td>\n",
       "      <td>[dear local newspaper, i think effects compute...</td>\n",
       "      <td>[dear, local, newspaper, i, think, effects, co...</td>\n",
       "      <td>112</td>\n",
       "      <td>7</td>\n",
       "      <td>624</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>78.28</td>\n",
       "      <td>8.13</td>\n",
       "      <td>75</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.248577</td>\n",
       "      <td>5.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>text_set1</td>\n",
       "      <td>dear i believe that using computers will benef...</td>\n",
       "      <td>[dear, i, believe, that, using, computers, wil...</td>\n",
       "      <td>[dear i believe that using computers will bene...</td>\n",
       "      <td>[dear, i, believe, that, using, computers, wil...</td>\n",
       "      <td>136</td>\n",
       "      <td>8</td>\n",
       "      <td>767</td>\n",
       "      <td>...</td>\n",
       "      <td>204</td>\n",
       "      <td>62.68</td>\n",
       "      <td>7.50</td>\n",
       "      <td>90</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>0.312173</td>\n",
       "      <td>6.014722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>text_set1</td>\n",
       "      <td>dear, more and more people use computers, but ...</td>\n",
       "      <td>[dear, ,, more, and, more, people, use, comput...</td>\n",
       "      <td>[dear, more and more people use computers, but...</td>\n",
       "      <td>[dear, more, and, more, people, use, computers...</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "      <td>519</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>75.30</td>\n",
       "      <td>7.61</td>\n",
       "      <td>70</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.313247</td>\n",
       "      <td>5.671034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>text_set1</td>\n",
       "      <td>dear local newspaper, i have found that many e...</td>\n",
       "      <td>[dear, local, newspaper, ,, i, have, found, th...</td>\n",
       "      <td>[dear local newspaper, i have found that many ...</td>\n",
       "      <td>[dear, local, newspaper, i, have, found, that,...</td>\n",
       "      <td>161</td>\n",
       "      <td>8</td>\n",
       "      <td>922</td>\n",
       "      <td>...</td>\n",
       "      <td>246</td>\n",
       "      <td>59.53</td>\n",
       "      <td>8.65</td>\n",
       "      <td>107</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.9891</td>\n",
       "      <td>0.237326</td>\n",
       "      <td>6.175490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>text_set1</td>\n",
       "      <td>dear i know having computers has a positive ef...</td>\n",
       "      <td>[dear, i, know, having, computers, has, a, pos...</td>\n",
       "      <td>[dear i know having computers has a positive e...</td>\n",
       "      <td>[dear, i, know, having, computers, has, a, pos...</td>\n",
       "      <td>153</td>\n",
       "      <td>9</td>\n",
       "      <td>823</td>\n",
       "      <td>...</td>\n",
       "      <td>225</td>\n",
       "      <td>71.14</td>\n",
       "      <td>6.85</td>\n",
       "      <td>90</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>0.244275</td>\n",
       "      <td>5.864906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set   text_set  \\\n",
       "0         1          1  text_set1   \n",
       "1         2          1  text_set1   \n",
       "2         3          1  text_set1   \n",
       "3         4          1  text_set1   \n",
       "4         5          1  text_set1   \n",
       "\n",
       "                                                text  \\\n",
       "0  dear local newspaper, i think effects computer...   \n",
       "1  dear i believe that using computers will benef...   \n",
       "2  dear, more and more people use computers, but ...   \n",
       "3  dear local newspaper, i have found that many e...   \n",
       "4  dear i know having computers has a positive ef...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [dear, local, newspaper, ,, i, think, effects,...   \n",
       "1  [dear, i, believe, that, using, computers, wil...   \n",
       "2  [dear, ,, more, and, more, people, use, comput...   \n",
       "3  [dear, local, newspaper, ,, i, have, found, th...   \n",
       "4  [dear, i, know, having, computers, has, a, pos...   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [dear local newspaper, i think effects compute...   \n",
       "1  [dear i believe that using computers will bene...   \n",
       "2  [dear, more and more people use computers, but...   \n",
       "3  [dear local newspaper, i have found that many ...   \n",
       "4  [dear i know having computers has a positive e...   \n",
       "\n",
       "                                   word_tokens_clean  word_count  sent_count  \\\n",
       "0  [dear, local, newspaper, i, think, effects, co...         112           7   \n",
       "1  [dear, i, believe, that, using, computers, wil...         136           8   \n",
       "2  [dear, more, and, more, people, use, computers...          91           7   \n",
       "3  [dear, local, newspaper, i, have, found, that,...         161           8   \n",
       "4  [dear, i, know, having, computers, has, a, pos...         153           9   \n",
       "\n",
       "   char_count  ...  syllabus_count  FleKin_score  DalCha_score  \\\n",
       "0         624  ...             152         78.28          8.13   \n",
       "1         767  ...             204         62.68          7.50   \n",
       "2         519  ...             136         75.30          7.61   \n",
       "3         922  ...             246         59.53          8.65   \n",
       "4         823  ...             225         71.14          6.85   \n",
       "\n",
       "   unique_word_count    neg    neu    pos  compound  cohesion  AoA_score  \n",
       "0                 75  0.000  0.847  0.153    0.9550  0.248577   5.925000  \n",
       "1                 90  0.015  0.787  0.198    0.9678  0.312173   6.014722  \n",
       "2                 70  0.016  0.670  0.314    0.9896  0.313247   5.671034  \n",
       "3                107  0.024  0.762  0.214    0.9891  0.237326   6.175490  \n",
       "4                 90  0.000  0.816  0.184    0.9828  0.244275   5.864906  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time() # takes about 30 seconds, mostly on the cohesion score calculation\n",
    "\n",
    "# content feature extraction \n",
    "## sentiments (4 metrics)\n",
    "## cohesion score, calculated as cosine similarity\n",
    "## Age of Acquisision score, mapped from the Kuperman dataset\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text_training_df[['neg', 'neu', 'pos', 'compound']] = text_training_df['text'].apply(lambda x: list(analyzer.polarity_scores(x).values())).apply(pd.Series)\n",
    "text_training_df['cohesion'] = text_training_df['text'].apply(lambda x: get_cohesion_score(x))\n",
    "text_training_df['AoA_score'] = text_training_df['word_tokens_clean'].apply(lambda x: get_avg_AoA(x))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Time taken for basic content feature extraction: {execution_time:.5f} seconds\")\n",
    "\n",
    "text_training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395f5359-22c5-4f02-b293-6bc592a52d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Text: His sentence have grammar issue.\n",
      "Number of Grammar Mistakes: 2\n"
     ]
    }
   ],
   "source": [
    "## this works but it works so badly\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# text = \"This sentense have grammar issue.\"\n",
    "# blob = TextBlob(text)\n",
    "# corrected_text = str(blob.correct())\n",
    "\n",
    "# print(\"Corrected Text:\", corrected_text)\n",
    "\n",
    "# # Counting grammar mistakes by comparing changes\n",
    "# original_words = text.split()\n",
    "# corrected_words = str(corrected_text).split()\n",
    "# num_mistakes = sum(1 for orig, corr in zip(original_words, corrected_words) if orig != corr)\n",
    "# print(\"Number of Grammar Mistakes:\", num_mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452f7d1-f8d5-496b-a824-b63c96b82114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
